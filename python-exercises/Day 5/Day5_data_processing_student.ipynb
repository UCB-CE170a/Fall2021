{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6g_vYQ_B2vu"
      },
      "source": [
        "# Introduction\n",
        "Infrastructures nowadays are generating data at such a rapid rate and great volume that it is sometimes impossible to handle them in traditional ways. Fortunately, programming tools such Python include many powerful libraries that can assist us during the data wrangling process.\n",
        "\n",
        "In this exercise, you will learn to read and analyze tabular and tree-structured data. Both of which are among the most popular types of inputs you will get for your work or research. Specifically, we will show how to handle `.csv`-like files using the Pandas library (https://pandas.pydata.org/) and perform tasks such as looping through each row and merging two pieces of data based on a common field. The Pandas library also works well with other tabular data formats, such as the `.xlsx` file. On the other hand, you can also work with semi-structured tree-like data (e.g., `.json` or `.xml`) using Python standard libraries (https://docs.python.org/3/library/). Pandas is not a Python standard library, thus it usually requires installation. The `json` and `xml` libraries are included in the Python standard libraries and do not require additional installation.\n",
        "\n",
        "Even though tools such as Pandas have made data analysis and processing a lot easier, in general data processing (cleaning, merging, validation, etc.), or sometimes even making sense of the content in the raw data, can still be very challenging and take the majority of the time in an infrastructure analysis project, due to incomplete or errors in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2SkQm5kBFCOa",
        "colab": {}
      },
      "source": [
        "### download data\n",
        "!wget \"https://raw.githubusercontent.com/UCB-CE170a/Fall2020/master/python-exercises/Day%205/hearst_euclid.csv\" -O hearst_euclid.csv\n",
        "!wget \"https://raw.githubusercontent.com/UCB-CE170a/Fall2020/master/traffic_data/berkeley_edges.csv\" -O berkeley_edges.csv\n",
        "!wget \"https://raw.githubusercontent.com/UCB-CE170a/Fall2020/master/python-exercises/Day%205/target_north_berkeley.osm\" -O target_north_berkeley.osm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iUxBHZh7B2vx"
      },
      "source": [
        "# 1. Tabular data (.csv, .xlsx)\n",
        "You are probably familiar with tabular data from using MS Excel or Google Spreadsheets. If we view each row in the data table as a separate record (e.g., the three rows in the table below represent three separate roads), then all records have the same set of attributes, (e.g., all roads have five attributes: ID, name, direction, start_intersection, and end_intersection). Because of this highly structured format, there is no need to store the name of attributes for each record, which saves a lot of space for large data files compared to `.json` or `.xml` file that will be introduced later.\n",
        "\n",
        "\n",
        "| ID | name          | direction      | start_intersection | end_intersection |\n",
        "|----|---------------|----------------|--------------------|------------------|\n",
        "| 1  | Hearst Avenue | East to West   | La Loma Avenue     | Oxford Street    |\n",
        "| 2  | Hearst Avenue | West to East   | Oxford Street      | La Loma Avenue   |\n",
        "| 3  | Euclid Avenue | South to North | Hearst Avenue      | Marin Avenue     |\n",
        "\n",
        "<span style=\"color:red\">Row 1 and 2 are the two directions of the same road between two common intersections. Two-way roads like this are usually represented by two separate road links in traffic analysis.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Wh4AxJsB2v0"
      },
      "source": [
        "## 1.1 Reading data\n",
        "In the folder of today's exercise, we provided an example `hearst_euclid.csv` file, which contains some information of the two streets on the Northside of the Berkeley campus. Feel free to download it and open in text editor or Excel while you are doing the exercise below. Now, let's read this file into Python and create a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eHccE9HxB2v3",
        "colab": {}
      },
      "source": [
        "# import the pandas library\n",
        "import pandas as pd \n",
        "\n",
        "# read the csv file by file name, \"hearst_euclid.csv\"\n",
        "df = \n",
        "\n",
        "# print the first few lines of the data\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ph8MVxL1B2wL"
      },
      "source": [
        "## 1.2 Exploration: shape and unique\n",
        "Now let's do some basic exploration:\n",
        "- How many rows/columns are there in the data?\n",
        "- How many different roads are in the data?\n",
        "\n",
        "Previously, we have read the csv file into the `df` variable, where `df` is short for `dataframe` (think dataframe like an excel table, or the two-dimensional array data frame in software `R`). You can name the variable whatever you like, but `df` is typical for variables that are pandas dataframes.\n",
        "\n",
        "Each pandas dataframe object has the `shape` property, which can be accessed using syntax such as `df.shape`. The first element of the `shape` property is the number of rows in the dataframe, and the second element is the number of columns.\n",
        "\n",
        "To focus on a particular column in a dataframe (which is called a `series`), you can access it using `df[column_name]`. Series has a method called `unique()` that returns the unique values in that column. Notice the `()` after the method, which is not there if we want to access properties such as `shape`.\n",
        "\n",
        "As you will soon find out, our data has two roads and each road has two directions. However, for each road-direction, the road is further broken down into smaller segments, thus the total number of records are larger than four."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TerKzKsiB2wN",
        "colab": {}
      },
      "source": [
        "# print the numbers of rows and columns in the data\n",
        "print( '# rows and columns',  )\n",
        "print( '# rows ',  )\n",
        "print( '# columns ',  )\n",
        "\n",
        "# print the names of unique values of the \"road_name\" column in the data\n",
        "print(  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uiBQi73TB2wa"
      },
      "source": [
        "## 1.3 Selecting data\n",
        "Sometimes, you may want to choose a subset of the data to work with. In this case, you will need to select the data based on some criteria (e.g, give me the `n`-th row or give me all records whose `name` is `Hearst_Ave`). Index in Pandas (as well as in Python) is 0-based, meaning the row id of the first record is 0, the row id of the second record is 1, and so forth. Selecting data in pandas is usually performed by `df.loc[]` (label based) or `df.iloc[]` (integer position based). Or use `df[column_name]` is also handy to retrieve a certain column in the dataframe. We will explain how to use them in the exercise below.\n",
        "\n",
        "If you want to index the data in other ways than introduced below, this page provides further information https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8WAiZ5Q-B2wb",
        "colab": {}
      },
      "source": [
        "# selecting the 1st row\n",
        "display(  ) ### based on index position\n",
        "\n",
        "# selecting the 10th row\n",
        "display(  )\n",
        "\n",
        "# selecting the 1st column in the first row\n",
        "display(  )\n",
        "\n",
        "# selecting the 10th row of column \"road_name\"\n",
        "display(  )\n",
        "\n",
        "# selecting all records with \"road_name\" equals to \"Euclid_Ave\"\n",
        "# and print the first two rows\n",
        "display(  )\n",
        "\n",
        "# selecting a subset of columns: \"start_node_id\", \"road_name\", \"direction\"\n",
        "# and print the first two rows\n",
        "display(  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rAgrmxTRB2wl"
      },
      "source": [
        "## 1.4 Looping through the data\n",
        "In the example data, there are two road names (`Hearst_Ave` and `Euclid_Ave`, while each road has two directions and multiple segments). We now want to create a new column, `road_and_direction`, which combines the first letter in the `road_name` column and `direction` column, e.g., `H_W` for the westward direction of Hearst avenue, or `E_S` for the southward direction of Euclid Avenue.\n",
        "\n",
        "There are different methods to apply the same function to each record in a dataframe. However, for beginners, it is easier to just think of this task as looping through each row of the dataframe (using `iterrows()` or `itertuples()`) and performing the same operations on each row. The performance is usually not worse than more sophisticated methods (unless you are doing simple algebra calculations, which can be vectorized and run much faster with NumPy, see https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6).\n",
        "\n",
        "We will introduce the `itertuples()` method here. It is typically used with a `for` loop. Inside the `for` loop, you can get the value under a specific column of the current row with `getattr()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VY978L-qB2wo",
        "colab": {}
      },
      "source": [
        "# first, create a new, empty list called `road_and_direction`\n",
        "# this is because usually we don't want to update the dataframe directy in a loop\n",
        "road_and_direction = []\n",
        "\n",
        "# then, iterate through the dataframe\n",
        "for row in df.itertuples():\n",
        "    \n",
        "    # get the first letter (remember, the index begins at 0 in Python) of the `road_name` column\n",
        "    r = \n",
        "    d = \n",
        "    r_d =   # you can concatenate strings with the `+` sign\n",
        "    road_and_direction.append( r_d )\n",
        "\n",
        "# create the road_and_direction column\n",
        "df['road_and_direction'] = \n",
        "\n",
        "# check results at random locations\n",
        "display(df.iloc[[0, 1, 15, -2, -1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPE3NhroB2w5"
      },
      "source": [
        "## 1.5 Merging\n",
        "You may want to combine two tables into a same dataframe. For example, our table looks like\n",
        "\n",
        "|     | start_node_id  | end_node_id   | road_name   | direction   | road_and_direction |\n",
        "|-----|---------------|----------------|-------------|-------------|--------------------|\n",
        "| 1   | 119           | 172            | Hearst_Ave  | Eastward    | H_E                |\n",
        "| 2   | 172           | 120            | Hearst_Ave  | Eastward    | H_E                |\n",
        "| ..  | ..            | ..             | ..          | ..          | ..                 |\n",
        "\n",
        "\n",
        "We also have another csv file, which looks like\n",
        "\n",
        "|     | start_node_id  | end_node_id     | length | lanes   | maxmph | geometry| .. |\n",
        "|-----|---------------|----------------|--------|---------|--------|---------|----|\n",
        "| 1   | 119          | 172           | 261  | 2     | 25.0   |LINESTRING (-122.2709149 37.87361069999999, -122.2708423 37.87361930000002, -122.2706021 37.87364759999999, -122.2697939 37.87374460000002, -122.2697939 37.87374460000002, -122.2693576 37.873797, -122.2691187 37.8738114, -122.2689644 37.87382910000001, -122.2687386 37.87385499999998, -122.26859415 37.87387224999998)| .. |\n",
        "| 2   | 172          | 120           | 124  | 2     | 25.0   |LINESTRING (-122.26859415 37.87387224999998, -122.2684485 37.87389039999998, -122.2683339 37.87390449999999, -122.2682235 37.87391820000001, -122.2679696 37.87394959999998, -122.2677958 37.87397109999998, -122.2676499 37.8739901, -122.2675517 37.87400279999999, -122.267495 37.87401010000002)| .. |\n",
        "| ..  | ..            | ..             | ..     | ..      | ..     |         | .. |\n",
        "\n",
        "Note that the two tables have two common column names, `start_node_id` and `end_node_id`. Actually, we also know (since we created the CSV files) that records with the same `start_node_id` and `end_node_id` refer to the same road segment in both CSV files. This makes it easier to combine the two tables into one dataframe. Specifically, we will use the `merge()` method to combine the tables. This (as well as many other pandas functions) works similarly to SQL commands if you are familiar with this database management language.\n",
        "\n",
        "There are four types of join/merge in Pandas. We want to keep all records in the first dataframe, `df`, so we will put it before the second dataframe, `large_df`, in the merge function.\n",
        "\n",
        "<img src=\"https://github.com/UCB-CE170a/Fall2020/blob/master/python-exercises/Day%205/types_of_merge.png?raw=1\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "[Source of image](https://www.datasciencemadesimple.com/join-merge-data-frames-pandas-python/)\n",
        "\n",
        "We will learn how to obtain the second csv file during the 7th lecture (September 17).\n",
        "While you are working on the exercise below, try to answer the following questions:\n",
        "- What is the shape of the large_df?\n",
        "- <span style=\"color:red\">What do you see if you go to https://www.openstreetmap.org/node/53055189?</span>\n",
        "- <span style=\"color:red\">Can you use the above method to quickly locate any road/intersection on OpenStreetMap?</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CiJxH5vB2w7",
        "colab": {}
      },
      "source": [
        "# read the csv file titled `berkeley_osm_edges.csv`, which has more information about the roads in Berkeley\n",
        "large_df = pd.read_csv('berkeley_edges.csv')\n",
        "\n",
        "# check some records\n",
        "display( large_df.loc[ \n",
        "            (large_df['start_node_id'].isin([119, 172])) & \n",
        "            (large_df['end_node_id'].isin([172, 120]))] \n",
        "       )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqhEtMd5mz61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "large_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGslctNBB2xF",
        "colab": {}
      },
      "source": [
        "# combine the two dataframes into a new one called `df_expand`\n",
        "# only keep road segments that are inside `df`\n",
        "df_expand = \n",
        "\n",
        "print( df_expand.shape )\n",
        "\n",
        "display( df_expand.head(2) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pzbZkMrZB2xN"
      },
      "source": [
        "## 1.6 Grouping statistics\n",
        "Another useful operation in Pandas is to aggregate/group records with a common column value and calculate group statistics. In the exercise below, we will practice how to obtain\n",
        "- The total length of Hearst Avenue and Euclid Avenue (sum up the `length` of segments).\n",
        "- How many segments are there for each road and directionality?\n",
        "\n",
        "We will use the `groupby()` method for this exercise. For more approaches to aggregate data, you can refer to https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vw3ImYxzB2xO",
        "colab": {}
      },
      "source": [
        "# first, group the records based on `road_name` using the `groupby()` method\n",
        "# second, calculate the total `length` of each group using the `agg( )` method on the groupped result\n",
        "display(  )\n",
        "\n",
        "# first, group the records that have the same `road_name` and `direction` using the `groupby()` method\n",
        "# second, count the number of records in each group use the `size()` method on the groupped result\n",
        "# then, use `to_frame()` to give the count column a suitable name\n",
        "display(  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b-2ldPNEB2xX"
      },
      "source": [
        "## 1.7 Saving files\n",
        "The dataframe can be saved to a CSV file, allowing you to work on it using other software (e.g., Excel, Kepler.gl)\n",
        "- <span style=\"color:red\">Try dragging the data to https://kepler.gl/demo</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6-zgteCuB2xZ",
        "colab": {}
      },
      "source": [
        "df_expand.to_csv('euclid_hearst_expand.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pG79otASB2xg"
      },
      "source": [
        "# 2. Semi-structured data (.json, .xml, .geojson)\n",
        "JSON (`.json`, `.geojson`) and XML (`.xml`, `.kml`) are also common data file extensions. These types of files have a tree-like hierarchical structure. For example, below is the first few lines of a JSON file containing information of a road network downloaded from the OpenStreetMap Overpass API (https://overpass-turbo.eu/):\n",
        "```\n",
        "{\n",
        "  \"version\": 0.6,\n",
        "  \"generator\": \"Overpass API\",\n",
        "  \"elements\": [road_1, road_2, ...],\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n",
        "In this section, we will learn some basic parsing of such data. Below are the raw road network data for North Berkeley. The extension is `.osm`, but it is still JSON-like. We will learn how to download such data from OpenStreetMaps in the 7th lecture on September 17.\n",
        "\n",
        "JSON files are read into Python as a dictionary. Try to print out each key (attribute) and value and see what is contained. Try to answer:\n",
        "- Which attribute contains the largest amount of information? Hint: check the type and length of values for each dictionary key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zk8tXSsB2xi",
        "colab": {}
      },
      "source": [
        "# we will handle the `.osm` file using the Python standard library `json`\n",
        "import json\n",
        "\n",
        "# read the file \"target_north_berkeley.osm\"\n",
        "osm_data = json.load( open('target_north_berkeley.osm') )\n",
        "\n",
        "# exploration\n",
        "print( 'JSON file (target*.osm) is loaded into Python as: ',  , '\\n' )\n",
        "print( 'JSON file has the following keys: ',  , '\\n' )\n",
        "print( \"The value of the 'version' key in the dictionary is: \",  , '\\n' )\n",
        "print( \"The value of the 'element' key in the dictionary has length: \",   , '\\n' )\n",
        "print( \"The first value of the 'element' key in the dictionary is: \",  , '\\n' )\n",
        "print( \"The last value of the 'element' key in the dictionary is: \",  , '\\n' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yiXMMGfoB2xp"
      },
      "source": [
        "## 2.1 Extracting and re-organizing the data\n",
        "After playing around with it, you will possibly realize that the information contained under the 'element' key that we are interested in the most. This part of the data contains the road and intersection information that we need to build a road network. Now let's separate the roads (also known as links/edges/ways) and intersections (also known as nodes). Also, we would only like to keep the following information for roads and intersections:\n",
        "- Intersection: id, lat and lon\n",
        "- Road: id, category and nodes forming the link\n",
        "\n",
        "Challenge: can you also extract the start and end coordinates of each road link?\n",
        "Hint: use dictionary lookup.\n",
        "\n",
        "Note: intersections are also called nodes. However, not all nodes in the `.osm` data are intersections. Some nodes that are not a real intersection (e.g., where streets intersect) are used to define the geometry position of, e.g., a curvature or a traffic signal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j8E4lRpaB2xq",
        "colab": {}
      },
      "source": [
        "### create an empty dictionary to hold node information\n",
        "all_nodes = {}\n",
        "\n",
        "# this is an example of a road node in the original data\n",
        "print( osm_data['elements'][0], '\\n' )\n",
        "\n",
        "### loop through the record and add new roads to the `all_nodes` dictionary\n",
        "for \n",
        "    \n",
        "print('The data contains {} nodes'.format(  ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gIA2En0CB2xz",
        "colab": {}
      },
      "source": [
        "### create an empty dictionary to hold road link information\n",
        "all_links = {}\n",
        "\n",
        "### this is an example of a road link in the original data\n",
        "print( osm_data['elements'][-1], '\\n')\n",
        "\n",
        "### loop through the record and add new links to the `all_links` dictionary\n",
        "for \n",
        "    \n",
        "print('it includes {} ways'.format(  ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "73UdwO7mB2x8"
      },
      "source": [
        "## 2.2 Saving the data\n",
        "After parsing the JSON data, we would like to save our results to output files. These can be done using the `dump()` method. After this, try opening the output files in a text editor and see if the results are what you expect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2HZ40H--B2x9",
        "colab": {}
      },
      "source": [
        "with open('nodes.json', 'w') as outfile:\n",
        "    json.dump(all_nodes, outfile, indent=2)\n",
        "\n",
        "with open('links.json', 'w') as outfile:\n",
        "    json.dump(all_links, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EixNhW2HB2yK",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}